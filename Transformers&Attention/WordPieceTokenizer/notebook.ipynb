{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## WordPiece Tokenizer Basic\n",
    "\n",
    "### Описание задачи\n",
    "\n",
    "WordPiece — это алгоритм субсловной токенизации, похожий на BPE, но с другим принципом объединения.\n",
    "WordPiece использует языковую модель для оценки возможных слияний, выбирая такие пары, которые максимизируют правдоподобие обучающих данных.\n",
    "\n",
    "WordPiece используется в BERT и многих моделях Google. Он создает субсловные токены, которые начинаются с `##`, чтобы показать, что это не начало слова.\n",
    "\n",
    "### Зачем нужен WordPiece\n",
    "\n",
    "WordPiece важен, потому что:\n",
    "\n",
    "- **BERT**: используется в моделях BERT\n",
    "- **Google models**: стандарт для моделей обработки текста Google\n",
    "- **Subword units**: эффективно обрабатывает неизвестные слова\n",
    "- **Production**: применяется в продакшн‑системах\n",
    "- **OOV handling**: лучше обрабатывает слова вне словаря, чем токенизация по словам\n",
    "\n",
    "### Математическое описание (в общих чертах)\n",
    "\n",
    "Алгоритм WordPiece:\n",
    "\n",
    "1. **Инициализация**: начать с токенов на уровне символов.\n",
    "2. **Оценка слияний**: оценивать возможные слияния с помощью правдоподобия языковой модели.\n",
    "3. **Слияние**: сливать пару, которая дает наибольший прирост правдоподобия.\n",
    "4. **Повторение**: повторять, пока не будет достигнут целевой размер словаря.\n",
    "\n",
    "Ключевое отличие от BPE:\n",
    "\n",
    "- BPE: объединяет наиболее частую пару.\n",
    "- WordPiece: объединяет пару, дающую максимальный рост правдоподобия.\n",
    "\n",
    "---\n",
    "\n",
    "## Задание\n",
    "\n",
    "Реализовать функцию `wordpiece_tokenize(text, vocab)`.\n",
    "\n",
    "**Входные данные:**\n",
    "\n",
    "- `text`: входная строка текста;\n",
    "- `vocab`: множество (`set`) или словарь (`dict`) допустимых WordPiece‑токенов.\n",
    "\n",
    "**Выходные данные:**\n",
    "\n",
    "- список строк‑токенов.\n",
    "\n",
    "### Требуемый алгоритм (упрощённая версия)\n",
    "\n",
    "Реализовать жадную токенизацию по правилу «самое длинное совпадение»:\n",
    "\n",
    "1. На каждом шаге брать **самое длинное возможное совпадение** с начала текущей подстроки.\n",
    "2. Если совпадение найдено в словаре `vocab`, добавить этот токен в результат и продолжить с остатком строки.\n",
    "3. Если совпадения нет, добавить токен для неизвестного слова (например, `\"<UNK>\"`).\n",
    "4. Вернуть список токенов.\n",
    "\n",
    "Это упрощенный жадный вариант; полный WordPiece использует более сложное сопоставление.\n",
    "\n",
    "### Ограничения и требования\n",
    "\n",
    "- Использовать чистый Python.\n",
    "- Реализовать жадную токенизацию с выбором **максимально длинного совпадения**.\n",
    "- Корректно проверять токены на наличие в словаре `vocab`.\n",
    "- Обязательно обрабатывать неизвестные токены (`<UNK>` или аналог).\n",
    "- Вернуть список строк‑токенов.\n",
    "\n",
    "---\n",
    "\n",
    "## Пример\n",
    "\n",
    "```python\n",
    "text = \"unhappiness\"\n",
    "vocab = {\n",
    "    \"un\",\n",
    "    \"##happy\",\n",
    "    \"##ness\",\n",
    "    \"un##happy\",\n",
    "    \"un##happy##ness\",\n",
    "    \"<UNK>\"\n",
    "}\n",
    "\n",
    "tokens = wordpiece_tokenize(text, vocab)\n",
    "# Возможные корректные ответы:\n",
    "# [\"un##happy##ness\"]\n",
    "# или [\"un\", \"##happy\", \"##ness\"]\n"
   ],
   "id": "e83088b7240e6dfd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def wordpiece_tokenize(text, vocab, unk_token=\"<UNK>\"):\n",
    "    if isinstance(vocab, dict): vocab = set(vocab.keys())\n",
    "    else: vocab = set(vocab)\n",
    "    output = []\n",
    "\n",
    "    for word in text.strip().split():\n",
    "        start, failed = 0, False\n",
    "        word_tokens = []\n",
    "\n",
    "        while start < len(word):\n",
    "            best = None\n",
    "            for end in range(len(word), start, -1):\n",
    "                piece = word[start:end]\n",
    "                token = piece if start == 0 else \"##\" + piece\n",
    "                if token in vocab:\n",
    "                    best = token\n",
    "                    break\n",
    "\n",
    "            if best is None:\n",
    "                failed = True\n",
    "                break\n",
    "            word_tokens.append(best)\n",
    "            start += len(best) if not best.startswith(\"##\") else len(best) - 2\n",
    "        if failed: output.append(unk_token)\n",
    "        else: output.extend(word_tokens)\n",
    "    return output"
   ],
   "id": "823869d44788c3c6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
