{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 29781,
     "databundleVersionId": 2887556,
     "sourceType": "competition"
    },
    {
     "sourceId": 2484624,
     "sourceType": "datasetVersion",
     "datasetId": 1169793
    }
   ],
   "dockerImageVersionId": 31153,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "id": "adjusted-crisis",
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Введение #\n",
    "\n",
    "Линейная регрессия отлично экстраполирует тренды, но не умеет учить взаимодействия. XGBoost отлично учит взаимодействия, но не умеет экстраполировать тренды. В этом уроке мы научимся создавать «гибридные» прогнозировщики, которые комбинируют дополняющие друг друга алгоритмы обучения и позволяют сильным сторонам одного компенсировать слабости другого.\n",
    "\n",
    "# Компоненты и остатки #\n",
    "\n",
    "Чтобы проектировать эффективные гибриды, нам нужно лучше понимать, как устроены временные ряды. До сих пор мы изучили три паттерна зависимости: тренд, сезонность и циклы. Многие временные ряды можно хорошо описать аддитивной моделью из этих трёх компонентов плюс некоторой по сути непредсказуемой, полностью случайной *ошибки*:\n",
    "\n",
    "```\n",
    "series = trend + seasons + cycles + error\n",
    "```\n",
    "\n",
    "Каждый из терминов в этой модели мы называем **компонентом** временного ряда.\n",
    "\n",
    "**Остатки** модели — это разница между целевой переменной, на которой модель обучалась, и её предсказаниями — другими словами, разница между фактической кривой и аппроксимированной. Если построить остатки против признака, получится «остаточная» часть цели — то, чему модель не научилась по этому признаку.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/mIeeaBD.png\" width=700, alt=\"\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Разница между целевым рядом и предсказаниями (синим) даёт ряд остатков.\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "Слева на рисунке выше показан фрагмент ряда *Tunnel Traffic* и кривая тренда‑сезонности из Урока 3. Вычитание аппроксимированной кривой оставляет остатки — справа. Остатки содержат всё из *Tunnel Traffic*, чему тренд‑сезонная модель не научилась.\n",
    "\n",
    "Можно представить обучение компонент временного ряда как итеративный процесс: сначала выучить тренд и вычесть его из ряда, затем выучить сезонность на детрендированных остатках и вычесть сезоны, затем выучить циклы и вычесть циклы, и, наконец, остаётся только непредсказуемая ошибка.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/XGJuheO.png\" width=700, alt=\"\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Пошаговое обучение компонент ряда <em>Mauna Loa CO2</em>. Вычтите аппроксимированную кривую (синим) из ряда, чтобы получить ряд для следующего шага.\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "Сложив все выученные компоненты, мы получим полную модель. По сути, именно это делает линейная регрессия, если обучить её на полном наборе признаков, моделирующих тренд, сезоны и циклы.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/HZEhuHF.png\" width=600, alt=\"\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Сложите выученные компоненты и получите полную модель.\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "# Гибридное прогнозирование с остатками #\n",
    "\n",
    "В предыдущих уроках мы использовали один алгоритм (линейную регрессию), чтобы выучить все компоненты сразу. Но возможно использовать один алгоритм для части компонентов и другой — для остальных. Так можно выбирать лучший алгоритм для каждой компоненты. Для этого один алгоритм обучают на исходном ряде, а второй — на ряде остатков.\n",
    "\n",
    "В деталях процесс такой:\n",
    "```\n",
    "# 1. Обучение и предсказание первой моделью\n",
    "model_1.fit(X_train_1, y_train)\n",
    "y_pred_1 = model_1.predict(X_train)\n",
    "\n",
    "# 2. Обучение и предсказание второй моделью на остатках\n",
    "model_2.fit(X_train_2, y_train - y_pred_1)\n",
    "y_pred_2 = model_2.predict(X_train_2)\n",
    "\n",
    "# 3. Сложение для итоговых предсказаний\n",
    "y_pred = y_pred_1 + y_pred_2\n",
    "```\n",
    "\n",
    "Обычно мы будем использовать разные наборы признаков (`X_train_1` и `X_train_2` выше) в зависимости от того, чему мы хотим научить каждую модель. Например, если первой моделью мы обучаем тренд, то для второй модели обычно не нужен признак тренда.\n",
    "\n",
    "Хотя можно использовать больше двух моделей, на практике это не слишком полезно. Наиболее распространённая стратегия построения гибридов — та, что описана выше: простой (обычно линейный) алгоритм, за которым следует сложный нелинейный обучатель вроде GBDT или глубокой нейросети. Простая модель обычно служит «помощником» для более мощного алгоритма.\n",
    "\n",
    "### Проектирование гибридов\n",
    "\n",
    "Существует много способов комбинировать ML‑модели помимо описанного в уроке. Но успешное комбинирование требует понимания того, как работают эти алгоритмы.\n",
    "\n",
    "В целом регрессионные алгоритмы могут делать прогнозы двумя способами: либо преобразуя *признаки*, либо преобразуя *цель*. Алгоритмы, преобразующие признаки, учат некоторую математическую функцию, которая берёт признаки на вход и затем комбинирует и преобразует их, чтобы получить выход, соответствующий целевым значениям в обучающем наборе. К таким относятся линейная регрессия и нейросети.\n",
    "\n",
    "Алгоритмы, преобразующие цель, используют признаки, чтобы группировать целевые значения в обучающем наборе, и предсказывают, усредняя значения внутри группы; набор признаков лишь указывает, какую группу усреднять. К таким относятся деревья решений и k‑ближайшие соседи.\n",
    "\n",
    "Важно следующее: преобразователи признаков обычно могут **экстраполировать** целевые значения за пределы обучающего набора при наличии подходящих признаков, но прогнозы преобразователей цели всегда ограничены диапазоном обучающего набора. Если временной «дамми‑признак» продолжает считать шаги времени, линейная регрессия продолжает рисовать линию тренда. При том же временном признаке дерево решений будет предсказывать тренд, заданный последним шагом обучающих данных, вечно в будущее. *Деревья решений не умеют экстраполировать тренды.* Случайные леса и градиентный бустинг над деревьями (например XGBoost) — это ансамбли деревьев решений, так что они тоже не умеют экстраполировать тренды.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/ZZtfuFJ.png\" width=600, alt=\"\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Дерево решений не сможет экстраполировать тренд за пределы обучающего набора.\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "Эта разница и мотивирует гибрид в этом уроке: использовать линейную регрессию для экстраполяции тренда, преобразовать *цель*, чтобы удалить тренд, и применить XGBoost к детрендированным остаткам. Чтобы «гибридизировать» нейросеть (преобразователь признаков), можно, наоборот, добавить предсказания другой модели как признак — нейросеть включит их в свои предсказания. Метод обучения на остатках — это тот же метод, который использует алгоритм градиентного бустинга, поэтому такие гибриды мы будем называть **бустинговыми**; метод использования предсказаний как признаков называется «стэкингом», поэтому такие гибриды мы будем называть **стэкинговыми**.\n",
    "\n",
    "<blockquote style=\"margin-right:auto; margin-left:auto; background-color: #ebf9ff; padding: 1em; margin:24px;\">\n",
    "<strong>Победные гибриды из соревнований Kaggle</strong>\n",
    "    <p>Для вдохновения — несколько лучших решений из прошлых конкурсов:</p>\n",
    "<ul>\n",
    "    <li><a href=\"https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/discussion/8125\">STL, бустинг поверх экспоненциального сглаживания</a> — Walmart Recruiting — Store Sales Forecasting</li>\n",
    "    <li><a href=\"https://www.kaggle.com/c/rossmann-store-sales/discussion/17896\">ARIMA и экспоненциальное сглаживание, бустинг поверх GBDT</a> — Rossmann Store Sales</li> \n",
    "    <li><a href=\"https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/39395\">Ансамбль стэкинговых и бустинговых гибридов</a> — Web Traffic Time Series Forecasting</li>\n",
    "    <li><a href=\"https://github.com/Mcompetitions/M4-methods/blob/slaweks_ES-RNN/118%20-%20slaweks17/ES_RNN_SlawekSmyl.pdf\">Экспоненциальное сглаживание, стэкинг с LSTM‑нейросетью</a> — M4 (не Kaggle)</li>\n",
    "</ul>\n",
    "</blockquote>\n",
    "\n",
    "# Пример — US Retail Sales #\n",
    "\n",
    "Датасет [*US Retail Sales*](https://www.census.gov/retail/index.html) содержит месячные данные продаж по различным розничным индустриям США за 1992–2019 годы, собранные Бюро переписи США. Наша цель — прогнозировать продажи в 2016–2019 годах по данным предыдущих лет. Помимо гибрида «линейная регрессия + XGBoost», мы также посмотрим, как подготовить датасет временного ряда для использования с XGBoost."
   ],
   "metadata": {}
  },
  {
   "id": "alternative-increase",
   "cell_type": "code",
   "source": "\nfrom pathlib import Path\nfrom warnings import simplefilter\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\nfrom xgboost import XGBRegressor\n\n\nsimplefilter(\"ignore\")\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\n    \"figure\",\n    autolayout=True,\n    figsize=(11, 4),\n    titlesize=18,\n    titleweight='bold',\n)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=16,\n    titlepad=10,\n)\nplot_params = dict(\n    color=\"0.75\",\n    style=\".-\",\n    markeredgecolor=\"0.25\",\n    markerfacecolor=\"0.25\",\n)\n\ndata_dir = Path(\"../input/ts-course-data/\")\nindustries = [\"BuildingMaterials\", \"FoodAndBeverage\"]\nretail = pd.read_csv(\n    data_dir / \"us-retail-sales.csv\",\n    usecols=['Month'] + industries,\n    parse_dates=['Month'],\n    index_col='Month',\n).to_period('D').reindex(columns=industries)\nretail = pd.concat({'Sales': retail}, names=[None, 'Industries'], axis=1)\n\nretail.head()",
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "burning-separate",
   "cell_type": "markdown",
   "source": [
    "Сначала используем модель линейной регрессии, чтобы выучить тренд в каждом ряду. Для демонстрации возьмём квадратичный тренд (порядок 2). (Код здесь по сути такой же, как в предыдущих уроках.) Хотя аппроксимация не идеальна, этого будет достаточно для наших целей."
   ],
   "metadata": {}
  },
  {
   "id": "smaller-marijuana",
   "cell_type": "code",
   "source": "\ny = retail.copy()\n\n# Create trend features\ndp = DeterministicProcess(\n    index=y.index,  # dates from the training data\n    constant=True,  # the intercept\n    order=2,        # quadratic trend\n    drop=True,      # drop terms to avoid collinearity\n)\nX = dp.in_sample()  # features for the training data\n\n# Test on the years 2016-2019. It will be easier for us later if we\n# split the date index instead of the dataframe directly.\nidx_train, idx_test = train_test_split(\n    y.index, test_size=12 * 4, shuffle=False,\n)\nX_train, X_test = X.loc[idx_train, :], X.loc[idx_test, :]\ny_train, y_test = y.loc[idx_train], y.loc[idx_test]\n\n# Fit trend model\nmodel = LinearRegression(fit_intercept=False)\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_fit = pd.DataFrame(\n    model.predict(X_train),\n    index=y_train.index,\n    columns=y_train.columns,\n)\ny_pred = pd.DataFrame(\n    model.predict(X_test),\n    index=y_test.index,\n    columns=y_test.columns,\n)\n\n# Plot\naxs = y_train.plot(color='0.25', subplots=True, sharex=True)\naxs = y_test.plot(color='0.25', subplots=True, sharex=True, ax=axs)\naxs = y_fit.plot(color='C0', subplots=True, sharex=True, ax=axs)\naxs = y_pred.plot(color='C3', subplots=True, sharex=True, ax=axs)\nfor ax in axs: ax.legend([])\n_ = plt.suptitle(\"Trends\")",
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "waiting-cookie",
   "cell_type": "markdown",
   "source": [
    "Хотя алгоритм линейной регрессии способен на многовыходную регрессию, алгоритм XGBoost — нет. Чтобы предсказывать несколько рядов одновременно с помощью XGBoost, мы преобразуем эти ряды из *широкого* формата (по одному ряду в колонке) в *длинный* формат (ряды индексируются категориями по строкам)."
   ],
   "metadata": {}
  },
  {
   "id": "blessed-wright",
   "cell_type": "code",
   "source": "# The `stack` method converts column labels to row labels, pivoting from wide format to long\nX = retail.stack()  # pivot dataset wide to long\ndisplay(X.head())\ny = X.pop('Sales')  # grab target series",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "infinite-australia",
   "cell_type": "markdown",
   "source": [
    "Чтобы XGBoost мог отличать наши два временных ряда, мы превратим метки строк для `'Industries'` в категориальный признак с кодированием меток. Также создадим признак годовой сезонности, взяв номера месяцев из временного индекса."
   ],
   "metadata": {}
  },
  {
   "id": "after-taylor",
   "cell_type": "code",
   "source": "# Turn row labels into categorical feature columns with a label encoding\nX = X.reset_index('Industries')\n# Label encoding for 'Industries' feature\nfor colname in X.select_dtypes([\"object\", \"category\"]):\n    X[colname], _ = X[colname].factorize()\n\n# Label encoding for annual seasonality\nX[\"Month\"] = X.index.month  # values are 1, 2, ..., 12\n\n# Create splits\nX_train, X_test = X.loc[idx_train, :], X.loc[idx_test, :]\ny_train, y_test = y.loc[idx_train], y.loc[idx_test]",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "capable-conviction",
   "cell_type": "markdown",
   "source": [
    "Теперь преобразуем предсказания тренда, полученные ранее, в длинный формат и вычтем их из исходных рядов. Так мы получим детрендированные (остаточные) ряды, которые XGBoost сможет выучить."
   ],
   "metadata": {}
  },
  {
   "id": "confidential-ready",
   "cell_type": "code",
   "source": "# Pivot wide to long (stack) and convert DataFrame to Series (squeeze)\ny_fit = y_fit.stack().squeeze()    # trend from training set\ny_pred = y_pred.stack().squeeze()  # trend from test set\n\n# Create residuals (the collection of detrended series) from the training set\ny_resid = y_train - y_fit\n\n# Train XGBoost on the residuals\nxgb = XGBRegressor()\nxgb.fit(X_train, y_resid)\n\n# Add the predicted residuals onto the predicted trends\ny_fit_boosted = xgb.predict(X_train) + y_fit\ny_pred_boosted = xgb.predict(X_test) + y_pred",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "focused-intention",
   "cell_type": "markdown",
   "source": [
    "Аппроксимация выглядит довольно хорошо, хотя видно, что тренд, выученный XGBoost, хорош ровно настолько, насколько хорош тренд, выученный линейной регрессией — в частности, XGBoost не смог компенсировать плохо подогнанный тренд в ряду `'BuildingMaterials'`."
   ],
   "metadata": {}
  },
  {
   "id": "fiscal-operation",
   "cell_type": "code",
   "source": "\naxs = y_train.unstack(['Industries']).plot(\n    color='0.25', figsize=(11, 5), subplots=True, sharex=True,\n    title=['BuildingMaterials', 'FoodAndBeverage'],\n)\naxs = y_test.unstack(['Industries']).plot(\n    color='0.25', subplots=True, sharex=True, ax=axs,\n)\naxs = y_fit_boosted.unstack(['Industries']).plot(\n    color='C0', subplots=True, sharex=True, ax=axs,\n)\naxs = y_pred_boosted.unstack(['Industries']).plot(\n    color='C3', subplots=True, sharex=True, ax=axs,\n)\nfor ax in axs: ax.legend([])",
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "typical-bride",
   "cell_type": "markdown",
   "source": [
    "# Ваш ход #\n",
    "\n",
    "[**Прогнозируйте Store Sales**](https://www.kaggle.com/kernels/fork/19616007) с XGBoost‑гибридом и попробуйте другие комбинации ML‑алгоритмов."
   ],
   "metadata": {}
  },
  {
   "id": "ab07d760",
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*Есть вопросы или комментарии? Посетите [форум обсуждений курса](https://www.kaggle.com/learn/time-series/discussion), чтобы пообщаться с другими учащимися.*"
   ],
   "metadata": {}
  }
 ]
}