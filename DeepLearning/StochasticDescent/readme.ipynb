{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 1480608,
     "sourceType": "datasetVersion",
     "datasetId": 829369
    }
   ],
   "dockerImageVersionId": 30646,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Введение #\n",
    "\n",
    "В первых двух уроках мы научились строить полносвязные сети из стеков плотных слоев. При создании все веса сети инициализируются случайно — сеть пока ничего не «знает». В этом уроке мы увидим, как обучать нейронную сеть; мы посмотрим, как нейросети *учатся*.\n",
    "\n",
    "Как и во всех задачах машинного обучения, мы начинаем с набора обучающих данных. Каждый пример в обучающих данных состоит из признаков (входов) и ожидаемой цели (выхода). Обучение сети означает настройку ее весов так, чтобы она могла преобразовывать признаки в цель. В наборе данных *80 Cereals*, например, нам нужна сеть, которая может по содержанию `'sugar'`, `'fiber'` и `'protein'` предсказать `'calories'` для каждого хлопья. Если мы успешно обучим сеть этому, ее веса должны в каком-то виде отражать связь между этими признаками и целью, выраженную в обучающих данных.\n",
    "\n",
    "Помимо обучающих данных нам нужны еще две вещи:\n",
    "- «Функция потерь», которая измеряет, насколько хороши предсказания сети.\n",
    "- «Оптимизатор», который подсказывает сети, как менять веса.\n",
    "\n",
    "# Функция потерь #\n",
    "\n",
    "Мы уже видели, как спроектировать архитектуру сети, но еще не обсуждали, как сказать сети, *какую* задачу решать. Это задача функции потерь.\n",
    "\n",
    "**Функция потерь** измеряет расхождение между истинным значением цели и значением, предсказанным моделью. \n",
    "\n",
    "Для разных задач подходят разные функции потерь. Мы рассматривали задачи **регрессии**, где нужно предсказать числовое значение — калории в *80 Cereals*, оценку в *Red Wine Quality*. Другие регрессионные задачи — прогноз цены дома или топливной эффективности автомобиля.\n",
    "\n",
    "Распространенная функция потерь для регрессии — **средняя абсолютная ошибка** или **MAE**. Для каждого предсказания `y_pred` MAE измеряет расхождение с истинной целью `y_true` как абсолютную разницу `abs(y_true - y_pred)`.\n",
    "\n",
    "Полная MAE-потеря на наборе данных — это среднее всех этих абсолютных разниц.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/VDcvkZN.png\" width=\"500\" alt=\"A graph depicting error bars from data points to the fitted line..\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Средняя абсолютная ошибка — это средняя длина отрезков между аппроксимирующей кривой и точками данных.\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "Помимо MAE, в регрессионных задачах часто используют среднеквадратичную ошибку (MSE) или loss Хьюбера (оба доступны в Keras).\n",
    "\n",
    "Во время обучения модель использует функцию потерь как ориентир для поиска правильных значений весов (меньшая потеря — лучше). Другими словами, функция потерь задает цель сети.\n",
    "\n",
    "# Оптимизатор — стохастический градиентный спуск #\n",
    "\n",
    "Мы описали задачу, которую должна решать сеть, но теперь нужно сказать, *как* ее решать. Это задача **оптимизатора**. Оптимизатор — это алгоритм, который изменяет веса, чтобы минимизировать потери.\n",
    "\n",
    "Практически все алгоритмы оптимизации, используемые в глубоком обучении, относятся к семейству **стохастического градиентного спуска**. Это итеративные алгоритмы, которые обучают сеть шаг за шагом. Один **шаг** обучения выглядит так:\n",
    "1. Берем часть обучающих данных и прогоняем через сеть, чтобы получить предсказания.\n",
    "2. Измеряем потери между предсказаниями и истинными значениями.\n",
    "3. Затем корректируем веса в направлении, которое уменьшает потери.\n",
    "\n",
    "И так снова и снова, пока потери не станут настолько малыми, насколько нужно (или пока не перестанут уменьшаться).\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/rFI1tIk.gif\" width=\"1600\" alt=\"Fitting a line batch by batch. The loss decreases and the weights approach their true values.\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Обучение нейронной сети с помощью стохастического градиентного спуска.\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "Каждая выборка обучающих данных на итерации называется **микропакетом** (или просто «batch»), а полный проход по всему набору данных — **эпохой**. Число эпох — это сколько раз сеть увидит каждый обучающий пример.\n",
    "\n",
    "Анимация показывает, как линейная модель из Урока 1 обучается с помощью SGD. Бледно-красные точки — весь обучающий набор, а ярко-красные — минибатчи. Каждый раз, когда SGD видит новый минибатч, он сдвигает веса (`w` — наклон и `b` — пересечение с осью y) к их правильным значениям на этом батче. Батч за батчем прямая в итоге сходится к наилучшему приближению. Видно, что потери уменьшаются по мере того, как веса приближаются к истинным значениям.\n",
    "\n",
    "## Скорость обучения и размер батча ##\n",
    "\n",
    "Обратите внимание, что прямая смещается лишь немного в направлении каждого батча (вместо того чтобы сразу двигаться полностью). Размер этих сдвигов определяется **скоростью обучения**. Меньшая скорость обучения означает, что сети нужно увидеть больше минибатчей, прежде чем ее веса сойдутся к наилучшим значениям.\n",
    "\n",
    "Скорость обучения и размер минибатчей — два параметра, которые сильнее всего влияют на то, как идет обучение SGD. Их взаимодействие часто тонкое, и правильный выбор этих параметров не всегда очевиден. (Мы изучим эти эффекты в упражнении.)\n",
    "\n",
    "К счастью, для большинства задач не требуется длительный подбор гиперпараметров, чтобы получить удовлетворительные результаты. **Adam** — это алгоритм SGD с адаптивной скоростью обучения, поэтому он подходит для большинства задач без настройки параметров (в некотором смысле «самонастраивающийся»). Adam — отличный оптимизатор общего назначения.\n",
    "\n",
    "## Добавление функции потерь и оптимизатора ##\n",
    "\n",
    "После определения модели вы можете добавить функцию потерь и оптимизатор с помощью метода `compile` модели:\n",
    "\n",
    "```\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"mae\",\n",
    ")\n",
    "```\n",
    "\n",
    "Обратите внимание: мы можем указать функцию потерь и оптимизатор просто строкой. Вы также можете обращаться к ним напрямую через API Keras — например, если хотите настроить параметры — но для нас значения по умолчанию вполне подходят.\n",
    "\n",
    "<blockquote style=\"margin-right:auto; margin-left:auto; background-color: #ebf9ff; padding: 1em; margin:24px;\">\n",
    "    <strong>Что в имени?</strong><br>\n",
    "Градиент — это вектор, который указывает, в каком направлении должны изменяться веса. Точнее, он показывает, как нужно менять веса, чтобы потери менялись <em>быстрее всего</em>. Мы называем процесс градиентным <strong>спуском</strong>, потому что он использует градиент, чтобы <em>спускаться</em> по кривой потерь к минимуму. <strong>Стохастический</strong> означает «определяемый случаем». Наше обучение <em>стохастично</em>, потому что минибатчи — это <em>случайные выборки</em> из набора данных. Вот почему это называется SGD!\n",
    "</blockquote>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Пример — Red Wine Quality #\n",
    "\n",
    "Теперь мы знаем все, что нужно, чтобы начать обучать модели глубокого обучения. Посмотрим это в действии! Мы будем использовать набор данных *Red Wine Quality*.\n",
    "\n",
    "Этот набор данных состоит из физико-химических измерений примерно 1600 португальских красных вин. Также включена оценка качества каждого вина по результатам слепых дегустаций. Насколько хорошо мы можем предсказать субъективную оценку качества вина по этим измерениям?\n",
    "\n",
    "Всю подготовку данных мы поместили в следующую скрытую ячейку. Она не критична для дальнейшего, поэтому можете ее пропустить. Одна вещь, на которую стоит обратить внимание: мы масштабировали каждый признак в интервал $[0, 1]$. Как мы подробнее обсудим в Уроке 5, нейросети обычно работают лучше, когда входы находятся в общем масштабе.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\nimport pandas as pd\nfrom IPython.display import display\n\nred_wine = pd.read_csv('../input/dl-course-data/red-wine.csv')\n\n# Create training and validation splits\ndf_train = red_wine.sample(frac=0.7, random_state=0)\ndf_valid = red_wine.drop(df_train.index)\ndisplay(df_train.head(4))\n\n# Scale to [0, 1]\nmax_ = df_train.max(axis=0)\nmin_ = df_train.min(axis=0)\ndf_train = (df_train - min_) / (max_ - min_)\ndf_valid = (df_valid - min_) / (max_ - min_)\n\n# Split features and target\nX_train = df_train.drop('quality', axis=1)\nX_valid = df_valid.drop('quality', axis=1)\ny_train = df_train['quality']\ny_valid = df_valid['quality']",
   "metadata": {
    "_kg_hide-input": true,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Сколько входов должна иметь эта сеть? Мы можем узнать это, посмотрев на число столбцов в матрице данных. Не забудьте не включать здесь целевую переменную (`'quality'`) — только входные признаки.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(X_train.shape)",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Одиннадцать столбцов означает одиннадцать входов.\n",
    "\n",
    "Мы выбрали трехслойную сеть с более чем 1500 нейронами. Такая сеть должна уметь изучать достаточно сложные зависимости в данных.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    layers.Dense(512, activation='relu', input_shape=[11]),\n    layers.Dense(512, activation='relu'),\n    layers.Dense(512, activation='relu'),\n    layers.Dense(1),\n])",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Выбор архитектуры модели должен быть частью процесса. Начинайте с простого и используйте валидационную потерю как ориентир. Подробнее о разработке моделей вы узнаете в упражнениях.\n",
    "\n",
    "После определения модели мы компилируем ее с оптимизатором и функцией потерь.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "model.compile(\n    optimizer='adam',\n    loss='mae',\n)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь мы готовы начать обучение! Мы сказали Keras подавать оптимизатору 256 строк обучающих данных за раз (это `batch_size`) и сделать это 10 раз, проходя весь набор данных (это `epochs`).\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "history = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=256,\n    epochs=10,\n)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Вы увидите, что Keras будет обновлять информацию о потерях по мере обучения модели.\n",
    "\n",
    "Однако зачастую лучше смотреть на график потерь. Метод `fit` сохраняет историю потерь во время обучения в объекте `History`. Мы преобразуем эти данные в DataFrame Pandas, что упрощает построение графика.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\n\n# convert the training history to a dataframe\nhistory_df = pd.DataFrame(history.history)\n# use Pandas native plot method\nhistory_df['loss'].plot();",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Обратите внимание, как потери выходят на плато по мере прохождения эпох. Когда кривая потерь становится горизонтальной, это означает, что модель научилась всему, чему может, и нет смысла продолжать обучение на дополнительных эпохах.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ваш ход #\n",
    "\n",
    "Теперь [**используйте стохастический градиентный спуск**](https://www.kaggle.com/kernels/fork/11887330), чтобы обучить вашу сеть.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*Есть вопросы или комментарии? Посетите [форум обсуждений курса](https://www.kaggle.com/learn/intro-to-deep-learning/discussion), чтобы пообщаться с другими учащимися.*\n"
   ],
   "metadata": {}
  }
 ]
}