{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 1480608,
     "sourceType": "datasetVersion",
     "datasetId": 829369
    }
   ],
   "dockerImageVersionId": 30646,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Введение #\n",
    "\n",
    "В этом уроке мы посмотрим, как строить нейронные сети, способные изучать сложные зависимости, которыми славятся глубокие нейронные сети.\n",
    "\n",
    "Ключевая идея здесь — *модульность*: построение сложной сети из более простых функциональных единиц. Мы видели, как линейный элемент вычисляет линейную функцию — теперь посмотрим, как комбинировать и модифицировать такие единицы, чтобы моделировать более сложные зависимости.\n",
    "\n",
    "# Слои #\n",
    "\n",
    "Обычно нейронные сети организуют нейроны в **слои**. Когда мы объединяем линейные элементы с общим набором входов, получаем **dense**-слой.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/2MA4iMV.png\" width=\"300\" alt=\"A stack of three circles in an input layer connected to two circles in a dense layer.\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Dense-слой из двух линейных элементов, получающих два входа и смещение.\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "Можно думать о каждом слое в нейронной сети как о некотором относительно простом преобразовании. Через глубокий стек слоёв нейронная сеть может преобразовывать свои входы всё более сложными способами. В хорошо обученной нейронной сети каждый слой — это преобразование, которое приближает нас на шаг к решению.\n",
    "\n",
    "<blockquote style=\"margin-right:auto; margin-left:auto; background-color: #ebf9ff; padding: 1em; margin:24px;\">\n",
    "    <strong>Многие типы слоёв</strong><br>\n",
    "«Слой» в Keras — очень общее понятие. Слой может быть, по сути, любым видом <em>преобразования данных</em>. Многие слои, такие как <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D\">свёрточные</a> и <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN\">рекуррентные</a>, преобразуют данные с помощью нейронов и отличаются главным образом схемой связей. Другие используются для <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\">инженерии признаков</a> или просто <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Add\">простых арифметических операций</a>. Здесь целый мир слоёв — <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers\">посмотрите</a>!\n",
    "</blockquote>\n",
    "\n",
    "# Функция активации #\n",
    "\n",
    "Однако оказывается, что два dense-слоя подряд без ничего между ними не лучше одного dense-слоя. Сами по себе dense-слои никогда не выводят нас за пределы мира линий и плоскостей. Нам нужно что-то *нелинейное*. Нам нужны функции активации.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/OLSUEYT.png\" width=\"400\" alt=\" \">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Без функций активации нейронные сети могут выучивать только линейные зависимости. Чтобы аппроксимировать кривые, нам понадобятся функции активации.\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "**Функция активации** — это просто функция, которую мы применяем к каждому выходу слоя (его *активациям*). Самая распространённая — *rectifier* $max(0, x)$.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/aeIyAlF.png\" width=\"400\" alt=\"A graph of the rectifier function. The line y=x when x>0 and y=0 when x<0, making a 'hinge' shape like '_/'.\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "График rectifier-функции — это линия, у которой отрицательная часть «выпрямлена» в ноль. Применение функции к выходам нейрона даёт *изгиб* в данных, уводя нас от простых линий.\n",
    "\n",
    "Когда мы присоединяем rectifier к линейному элементу, получаем **rectified linear unit** или **ReLU**. (По этой причине обычно и называют rectifier-функцию «ReLU-функцией».) Применение активации ReLU к линейному элементу означает, что выход становится `max(0, w * x + b)`, что можно изобразить так:\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/eFry7Yu.png\" width=\"250\" alt=\"Diagram of a single ReLU. Like a linear unit, but instead of a '+' symbol we now have a hinge '_/'. \">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Выпрямленная линейная единица.\n",
    "</center></figcaption>\n",
    "</figure>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Стекование слоёв Dense #\n",
    "\n",
    "Теперь, когда у нас есть некоторая нелинейность, посмотрим, как можно стековать слои, чтобы получить сложные преобразования данных.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/Y5iwFQZ.png\" width=\"450\" alt=\"An input layer, two hidden layers, and a final linear layer.\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Стек dense-слоёв образует «полносвязную» сеть.\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "Слои перед выходным слоем иногда называют **скрытыми**, поскольку мы никогда не видим их выходы напрямую.\n",
    "\n",
    "Теперь обратите внимание, что последний (выходной) слой — линейный элемент (то есть без функции активации). Это делает сеть подходящей для задачи регрессии, где мы пытаемся предсказать произвольное числовое значение. Другие задачи (например, классификация) могут требовать функции активации на выходе.\n",
    "\n",
    "## Построение моделей Sequential ##\n",
    "\n",
    "Модель `Sequential`, которую мы использовали, соединяет список слоёв по порядку от первого к последнему: первый слой получает вход, последний слой выдаёт выход. Это создаёт модель, показанную на рисунке выше:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    # the hidden ReLU layers\n    layers.Dense(units=4, activation='relu', input_shape=[2]),\n    layers.Dense(units=3, activation='relu'),\n    # the linear output layer \n    layers.Dense(units=1),\n])",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Обязательно передавайте все слои вместе в списке, например `[layer, layer, layer, ...]`, а не отдельными аргументами. Чтобы добавить функции активации к слою, просто укажите её имя в аргументе `activation`.\n",
    "\n",
    "# Ваш ход #\n",
    "\n",
    "Теперь [**создайте глубокую нейронную сеть**](https://www.kaggle.com/kernels/fork/11887344) для набора данных *Concrete*."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*Есть вопросы или комментарии? Посетите [форум обсуждения курса](https://www.kaggle.com/learn/intro-to-deep-learning/discussion), чтобы пообщаться с другими учащимися.*"
   ],
   "metadata": {}
  }
 ]
}